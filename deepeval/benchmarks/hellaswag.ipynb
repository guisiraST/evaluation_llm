{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HellaSwag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HellaSwag เป็นเกณการประเมิน LLM โดยการเติมประโยคให้สมบูรณ์ โดยครอบคลุมในหลายสาขาวิชา มากกว่า 10000 รายการ \n",
    "\n",
    "### Arguments\n",
    "- [Optional] tasks: a list of tasks (HellaSwagTask enums), which specifies the subject areas for sentence completion evaluation. By default, this is set to all tasks. The list of HellaSwagTask enums can be found here.\n",
    "- [Optional] n_shots: the number of \"shots\" to use for few-shot learning. This is set to 10 by default and cannot exceed 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    ")\n",
    "azure_openai = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import HellaSwag\n",
    "from deepeval.benchmarks.tasks import HellaSwagTask\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = HellaSwag(\n",
    "    tasks=[HellaSwagTask.TRIMMING_BRANCHES_OR_HEDGES, HellaSwagTask.BATON_TWIRLING],\n",
    "    n_shots=5\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral_7b)\n",
    "print(benchmark.overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall_score for this benchmark ranges from 0 to 1, where 1 signifies perfect performance and 0 indicates no correct answers. The model's score, based on exact matching, is calculated by determining the proportion of multiple-choice sentence-completion questions for which the model produces the precise correct letter answer (e.g. 'A') in relation to the total number of questions.\n",
    "\n",
    "As a result, utilizing more few-shot prompts (n_shots) can greatly improve the model's robustness in generating answers in the exact correct format and boost the overall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
