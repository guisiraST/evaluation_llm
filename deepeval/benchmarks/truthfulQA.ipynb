{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruthfulQA \n",
    "\n",
    "TruthfulQA เป็นการประเมินความถูกต้องของโมเดลภาษาในการตอบคำถามอย่างตรงไปตรงมา โดยมีคำถามทั้งหมด 817 ข้อ ครอบคลุม 38 หัวข้อ เช่น สุขภาพ กฎหมาย การเงิน และการเมือง\n",
    "\n",
    "### Arguments\n",
    "There are two optional arguments when using the TruthfulQA benchmark:\n",
    "\n",
    "- [Optional] tasks: a list of tasks (TruthfulQATask enums), which specifies the subject areas for model evaluation. By default, this is set to all tasks. The complete list of TruthfulQATask enums can be found here.\n",
    "- [Optional] mode: a TruthfulQAMode enum that selects the evaluation mode. This is set to TruthfulQAMode.MC1 by default. deepeval currently supports 2 modes: MC1 and MC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    ")\n",
    "azure_openai = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import TruthfulQA\n",
    "from deepeval.benchmarks.tasks import TruthfulQATask\n",
    "from deepeval.benchmarks.modes import TruthfulQAMode\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "benchmark = TruthfulQA(\n",
    "    tasks=[TruthfulQATask.ADVERTISING, TruthfulQATask.FICTION],\n",
    "    mode=TruthfulQAMode.MC2\n",
    ")\n",
    "\n",
    "# Replace 'mistral_7b' with your own custom model\n",
    "benchmark.evaluate(model=mistral_7b)\n",
    "print(benchmark.overall_score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
