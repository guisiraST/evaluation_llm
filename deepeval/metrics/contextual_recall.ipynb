{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual Recall ประเมินควาแม่นยำของ RAG เพื่เปรียบเทียบระหว่าง retrieval_context ใกล้เคียงกับ expected_output \n",
    "\n",
    "### Required Arguments\n",
    "\n",
    "- input\n",
    "- actual_output\n",
    "- expected_output\n",
    "- retrieval_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.42, however version 0.21.45 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    ")\n",
    "azure_openai = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baf39cbb999410ca3f1897f6c4fdc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffe5b72a18f4639bd0cd780d1177506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the sentence in the expected output can be directly attributed to the 1st node in the retrieval context, indicating a perfect recall of information. Great job!\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Recall (score: 1.0, threshold: 0.7, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The score is 1.00 because the sentence perfectly matches the information in the 1st node in the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra cost.\n",
      "  - expected output: You are eligible for a 30 day full refund at no extra cost.\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra cost.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "ContextualRecallMetric: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/portalocker/utils.py:218: UserWarning: timeout has no effect in blocking mode\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics=[<deepeval.metrics.contextual_recall.contextual_recall.ContextualRecallMetric object at 0x7fc05c8e24c0>], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra cost.', expected_output='You are eligible for a 30 day full refund at no extra cost.', context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra cost.'])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualRecallMetric(\n",
    "    threshold=0.7,\n",
    "    model=azure_openai,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five optional parameters when creating a ContextualRecallMetric:\n",
    "- [Optional] threshold: a float representing the minimum passing threshold, defaulted to 0.5.\n",
    "- [Optional] model: a string specifying which of OpenAI's GPT models to use, OR any custom LLM model of type DeepEvalBaseLLM. Defaulted to 'gpt-4o'.\n",
    "- [Optional] include_reason: a boolean which when set to True, will include a reason for its evaluation score. Defaulted to True.\n",
    "- [Optional] strict_mode: a boolean which when set to True, enforces a binary metric score: 1 for perfection, 0 otherwise. It also overrides the current threshold and sets it to 1. Defaulted to False.\n",
    "- [Optional] async_mode: a boolean which when set to True, enables concurrent execution within the measure() method. Defaulted to Tru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Is It Calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](contextual_recall_formular.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ContextualRecallMetric ใช้ LLM ในการดึงข้อความทั้งหมดที่ปรากฏใน expected_output จากนั้นใช้ LLM เดียวกันในการจำแนกว่าแต่ละข้อความสามารถอ้างอิงไปยังโหนดใน retrieval_context ได้หรือไม่\n",
    "\n",
    "ข้อมูลเพิ่มเติม:\n",
    "- ใช้ expected_output แทน actual_output เพราะเราวัดคุณภาพของตัวดึงข้อมูล RAG สำหรับผลลัพธ์ที่คาดหวัง\n",
    "\n",
    "คะแนน recall เชิงบริบทที่สูงกว่าหมายถึงความสามารถที่มากขึ้นของระบบการดึงข้อมูลในการจับข้อมูลที่เกี่ยวข้องทั้งหมดจากชุดข้อมูลที่มีอยู่ในฐานความรู้ของคุณ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
