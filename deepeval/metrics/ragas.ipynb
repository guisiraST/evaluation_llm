{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS \n",
    "RAGAS เมทริกซ์จะประกอบไปด้วย 4 metrics เฉพาะ \n",
    "- RAGASAnswerRelevancyMetric\n",
    "- RAGASFaithfulnessMetric\n",
    "- RAGASContextualPrecisionMetric\n",
    "- RAGASContextualRecallMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS จะคล้ายกับ deepeval แต่ไม่สามารถให้เหตุผลได้ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Arguments\n",
    "- input\n",
    "- actual_output\n",
    "- expected_output\n",
    "- retrieval_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    ")\n",
    "azure_openai = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an AI language model created to assist and answer questions. How can I help you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_openai.generate(\"What the ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.42, however version 0.21.45 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de80b40b799944ebac6e820b4d6defcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddc04a2c65f41879813467f8ba99ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8336cb426eab412bac0638063e8e77e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790e83eb5e0a4e148a7d4ea6a2432526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/executor.py\", line 96, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"/usr/local/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/executor.py\", line 84, in _aresults\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "  File \"/usr/local/lib/python3.9/asyncio/tasks.py\", line 611, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/metrics/base.py\", line 123, in ascore\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/metrics/base.py\", line 119, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/metrics/_answer_relevance.py\", line 167, in _ascore\n",
      "    return self._calculate_score(answers, row)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/metrics/_answer_relevance.py\", line 139, in _calculate_score\n",
      "    cosine_sim = self.calculate_similarity(question, gen_questions)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/metrics/_answer_relevance.py\", line 113, in calculate_similarity\n",
      "    question_vec = np.asarray(self.embeddings.embed_query(question)).reshape(1, -1)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/ragas/embeddings/base.py\", line 55, in embed_query\n",
      "    return self.embeddings.embed_query(text)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/langchain_openai/embeddings/base.py\", line 546, in embed_query\n",
      "    return self.embed_documents([text])[0]\n",
      "  File \"/usr/local/lib/python3.9/site-packages/langchain_openai/embeddings/base.py\", line 517, in embed_documents\n",
      "    return self._get_len_safe_embeddings(texts, engine=engine)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/langchain_openai/embeddings/base.py\", line 333, in _get_len_safe_embeddings\n",
      "    response = self.client.create(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/openai/resources/embeddings.py\", line 113, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/openai/_base_client.py\", line 1208, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.9/site-packages/openai/_base_client.py\", line 897, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/openai/_base_client.py\", line 988, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: 77ba7a65********************af22. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m metric \u001b[38;5;241m=\u001b[39m RagasMetric(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, model\u001b[38;5;241m=\u001b[39mazure_model)\n\u001b[1;32m     15\u001b[0m test_case \u001b[38;5;241m=\u001b[39m LLMTestCase(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat if these shoes don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt fit?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     actual_output\u001b[38;5;241m=\u001b[39mactual_output,\n\u001b[1;32m     18\u001b[0m     expected_output\u001b[38;5;241m=\u001b[39mexpected_output,\n\u001b[1;32m     19\u001b[0m     retrieval_context\u001b[38;5;241m=\u001b[39mretrieval_context\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(metric\u001b[38;5;241m.\u001b[39mscore)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# or evaluate test cases in bulk\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/deepeval/metrics/ragas.py:388\u001b[0m, in \u001b[0;36mRagasMetric.measure\u001b[0;34m(self, test_case)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m capture_metric_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m--> 388\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m         score_breakdown[metric\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m score\n\u001b[1;32m    391\u001b[0m     ragas_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(score_breakdown\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(score_breakdown)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/deepeval/metrics/ragas.py:202\u001b[0m, in \u001b[0;36mRAGASAnswerRelevancyMetric.measure\u001b[0;34m(self, test_case)\u001b[0m\n\u001b[1;32m    199\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(data)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m capture_metric_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, _track\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_track):\n\u001b[0;32m--> 202\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     answer_relevancy_score \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_relevancy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuccess \u001b[38;5;241m=\u001b[39m answer_relevancy_score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/ragas/evaluation.py:250\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    248\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[1;32m    253\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[1;32m    254\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m    255\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/ragas/evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    230\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics.ragas import RagasMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = RagasMetric(threshold=0.5, model=azure_model)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error when running RagasMetric, So recommand to run on directly ragas lib modude instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGASAnswerRelevancyMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/datasets/load.py:1486: FutureWarning: The repository for explodinggradients/amnesty_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/explodinggradients/amnesty_qa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    eval: Dataset({\n",
       "        features: ['question', 'ground_truth', 'answer', 'contexts'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data\n",
    "from datasets import load_dataset\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "amnesty_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    eval: Dataset({\n",
      "        features: ['question', 'ground_truth', 'answer', 'contexts'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value, Sequence\n",
    "\n",
    "# Define the schema\n",
    "features = Features({\n",
    "    'question': Value('string'),\n",
    "    'ground_truth': Value('string'),\n",
    "    'answer': Value('string'),\n",
    "    'contexts': Sequence(Value('string')),\n",
    "})\n",
    "\n",
    "# Custom data for the dataset about Thailand\n",
    "data = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of Thailand?\",\n",
    "        \"ground_truth\": \"Bangkok\",\n",
    "        \"answer\": \"Bangkok\",\n",
    "        \"contexts\": [\"Bangkok, the capital of Thailand, is a large city known for its vibrant street life and cultural landmarks.\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When is the Songkran festival celebrated in Thailand?\",\n",
    "        \"ground_truth\": \"April 13-15\",\n",
    "        \"answer\": \"April 13-15\",\n",
    "        \"contexts\": [\"Songkran is the Thai New Year festival celebrated annually from April 13th to 15th, known for its water fights and cultural festivities.\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the official language of Thailand?\",\n",
    "        \"ground_truth\": \"Thai\",\n",
    "        \"answer\": \"Thai\",\n",
    "        \"contexts\": [\"Thai, also known as Siamese, is the official language of Thailand and the native language of the Thai people.\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which famous beach is located in Thailand?\",\n",
    "        \"ground_truth\": \"Phuket\",\n",
    "        \"answer\": \"Phuket\",\n",
    "        \"contexts\": [\"Phuket is a popular tourist destination in Thailand, known for its stunning beaches, clear waters, and vibrant nightlife.\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the main religion in Thailand?\",\n",
    "        \"ground_truth\": \"Buddhism\",\n",
    "        \"answer\": \"Buddhism\",\n",
    "        \"contexts\": [\"Buddhism is the main religion in Thailand, with about 95 percent of the population identifying as Buddhists.\"]\n",
    "    },\n",
    "    # Add more data as needed\n",
    "]\n",
    "\n",
    "# Create the dataset\n",
    "custom_dataset = Dataset.from_dict({\"question\": [d[\"question\"] for d in data],\n",
    "                                    \"ground_truth\": [d[\"ground_truth\"] for d in data],\n",
    "                                    \"answer\": [d[\"answer\"] for d in data],\n",
    "                                    \"contexts\": [d[\"contexts\"] for d in data]},\n",
    "                                   features=features)\n",
    "\n",
    "# If you need a DatasetDict, you can create it and add your dataset under the appropriate split\n",
    "custom_dataset_dict = DatasetDict({\"eval\": custom_dataset})\n",
    "\n",
    "# Display the dataset\n",
    "print(custom_dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "\n",
    "azure_configs = {\n",
    "    \"base_url\": settings.AZURE_OPENAI_ENDPOINT,\n",
    "    \"model_deployment\": None,\n",
    "    \"model_name\": \"gpt-35-turbo\",\n",
    "    \"embedding_deployment\": None,\n",
    "    \"embedding_name\": \"text-embedding-ada-002\",  # most likely\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "azure_model = AzureChatOpenAI(\n",
    "    openai_api_version=settings.OPENAI_API_VERSION,\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=azure_configs[\"model_deployment\"],\n",
    "    model=azure_configs[\"model_name\"],\n",
    "    validate_base_url=False,\n",
    ")\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=settings.OPENAI_API_VERSION,\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "    model=azure_configs[\"embedding_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87b4b8528ca428ab32cb08493ba3e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 1.0000, 'answer_relevancy': 0.9727, 'context_recall': 0.8000, 'context_precision': 1.0000, 'harmfulness': 0.0000}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics.critique import harmfulness\n",
    "\n",
    "# list of metrics we're going to use\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    harmfulness,\n",
    "]\n",
    "\n",
    "result = evaluate(\n",
    "    custom_dataset_dict[\"eval\"], metrics=metrics, llm=azure_model, embeddings=azure_embeddings\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>harmfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of Thailand?</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>[Bangkok, the capital of Thailand, is a large ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When is the Songkran festival celebrated in Th...</td>\n",
       "      <td>April 13-15</td>\n",
       "      <td>April 13-15</td>\n",
       "      <td>[Songkran is the Thai New Year festival celebr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the official language of Thailand?</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Thai</td>\n",
       "      <td>[Thai, also known as Siamese, is the official ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which famous beach is located in Thailand?</td>\n",
       "      <td>Phuket</td>\n",
       "      <td>Phuket</td>\n",
       "      <td>[Phuket is a popular tourist destination in Th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.876384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the main religion in Thailand?</td>\n",
       "      <td>Buddhism</td>\n",
       "      <td>Buddhism</td>\n",
       "      <td>[Buddhism is the main religion in Thailand, wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question ground_truth  \\\n",
       "0                   What is the capital of Thailand?      Bangkok   \n",
       "1  When is the Songkran festival celebrated in Th...  April 13-15   \n",
       "2         What is the official language of Thailand?         Thai   \n",
       "3         Which famous beach is located in Thailand?       Phuket   \n",
       "4             What is the main religion in Thailand?     Buddhism   \n",
       "\n",
       "        answer                                           contexts  \\\n",
       "0      Bangkok  [Bangkok, the capital of Thailand, is a large ...   \n",
       "1  April 13-15  [Songkran is the Thai New Year festival celebr...   \n",
       "2         Thai  [Thai, also known as Siamese, is the official ...   \n",
       "3       Phuket  [Phuket is a popular tourist destination in Th...   \n",
       "4     Buddhism  [Buddhism is the main religion in Thailand, wi...   \n",
       "\n",
       "   faithfulness  answer_relevancy  context_recall  context_precision  \\\n",
       "0           NaN          0.999999             1.0                1.0   \n",
       "1           1.0          0.987328             1.0                1.0   \n",
       "2           NaN          0.999998             1.0                1.0   \n",
       "3           NaN          0.876384             0.0                1.0   \n",
       "4           NaN          0.999999             1.0                1.0   \n",
       "\n",
       "   harmfulness  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = result.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ragas.io/en/stable/howtos/customisations/azure-openai.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
