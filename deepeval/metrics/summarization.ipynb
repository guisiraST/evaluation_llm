{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใช้ LLM เพื่อพิจารณาว่า output LLM สร้างการสรุปที่ถูกต้องตามข้อเท็จจริงหรือไม่ พร้อมทั้งรวมรายละเอียดที่จำเป็นจากข้อความต้นฉบับด้วย \n",
    "\n",
    "ส่วนประกอบสำคัญของ SummarizationMetric :\n",
    "- input\n",
    "- actual_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.42, however version 0.21.45 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "\n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    ")\n",
    "azure_openai = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original text to be summarized\n",
    "input = \"\"\"\n",
    "The 'coverage score' is calculated as the percentage of assessment questions\n",
    "for which both the summary and the original document provide a 'yes' answer. This\n",
    "method ensures that the summary not only includes key information from the original\n",
    "text but also accurately represents it. A higher coverage score indicates a\n",
    "more comprehensive and faithful summary, signifying that the summary effectively\n",
    "encapsulates the crucial points and details from the original content.\n",
    "\"\"\"\n",
    "\n",
    "# This is the summary, replace this with the actual output from your LLM application\n",
    "actual_output=\"\"\"\n",
    "The coverage score quantifies how well a summary captures and\n",
    "accurately represents key information from the original text,\n",
    "with a higher score indicating greater comprehensiveness.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047d4457087f4de59d19ae28c6387ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1f9650abec4ae684094a73c1c9590a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "The score is 0.50 because the summary accurately captures the main point about the coverage score calculation but does not explicitly mention how it quantifies how well a summary represents key information from the original text. It also fails to include any extra information not mentioned in the original text. It does not raise any questions that the original text can answer, so the summary is good in that aspect.\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Summarization (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: Custom Azure OpenAI Model, reason: The score is 0.33 because the summary accurately captures the information in the original text without any contradicting information or extra details. Additionally, it does not leave out any important questions that the original text can answer but not the summary., error: None)\n",
      "      - Alignment (score: 1.0)\n",
      "      - Coverage (score: 0.3333333333333333)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: \n",
      "The 'coverage score' is calculated as the percentage of assessment questions\n",
      "for which both the summary and the original document provide a 'yes' answer. This\n",
      "method ensures that the summary not only includes key information from the original\n",
      "text but also accurately represents it. A higher coverage score indicates a\n",
      "more comprehensive and faithful summary, signifying that the summary effectively\n",
      "encapsulates the crucial points and details from the original content.\n",
      "\n",
      "  - actual output: \n",
      "The coverage score quantifies how well a summary captures and\n",
      "accurately represents key information from the original text,\n",
      "with a higher score indicating greater comprehensiveness.\n",
      "\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "SummarizationMetric: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/portalocker/utils.py:218: UserWarning: timeout has no effect in blocking mode\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=False, metrics=[<deepeval.metrics.summarization.summarization.SummarizationMetric object at 0x7ff27ece9460>], input=\"\\nThe 'coverage score' is calculated as the percentage of assessment questions\\nfor which both the summary and the original document provide a 'yes' answer. This\\nmethod ensures that the summary not only includes key information from the original\\ntext but also accurately represents it. A higher coverage score indicates a\\nmore comprehensive and faithful summary, signifying that the summary effectively\\nencapsulates the crucial points and details from the original content.\\n\", actual_output='\\nThe coverage score quantifies how well a summary captures and\\naccurately represents key information from the original text,\\nwith a higher score indicating greater comprehensiveness.\\n', expected_output=None, context=None, retrieval_context=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import SummarizationMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "...\n",
    "\n",
    "test_case = LLMTestCase(input=input, actual_output=actual_output)\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=azure_openai,\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seven optional parameters when instantiating an SummarizationMetric class:\n",
    "\n",
    "- [Optional] threshold: the passing threshold, defaulted to 0.5.\n",
    "- [Optional] assessment_questions: a list of close-ended questions that can be answered with either a 'yes' or a 'no'. These are questions you want your summary to be able to ideally answer, and is especially helpful if you already know what a good summary for your use case looks like. If assessment_questions is not provided, we will generate a set of assessment_questions for you at evaluation time. The assessment_questions are used to calculate the coverage_score.\n",
    "- [Optional] n: the number of assessment questions to generate when assessment_questions is not provided. Defaulted to 5.\n",
    "- [Optional] model: a string specifying which of OpenAI's GPT models to use, OR any custom LLM model of type DeepEvalBaseLLM. Defaulted to 'gpt-4o'.\n",
    "- [Optional] include_reason: a boolean which when set to True, will include a reason for its evaluation score. Defaulted to True.\n",
    "- [Optional] strict_mode: a boolean which when set to True, enforces a strict evaluation criterion. In strict mode, the metric score becomes binary: a score of 1 indicates a perfect result, and any outcome less than perfect is scored as 0. Defaulted as False.\n",
    "- [Optional] async_mode: a boolean which when set to True, enables concurrent execution within the measure() method. Defaulted to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Is It Calculated?\n",
    "\n",
    "สูตรการคำนวณการหา summarization metric \n",
    "\n",
    "Summarization=min(Alignment Score,Coverage Score)\n",
    "\n",
    "- alignment_score ข้อมูลมี hallucinated หรือขัดแย้งกับ original text ไหม \n",
    "- coverage_score บทสรุปมีข้อความที่จำเป็นขากต้นฉบับหรือไม่ \n",
    "\n",
    "จากที่ดู alignment_score จะคล้ายกับการหา HallucinationMetric ในขณะที่ coverage_score เป็นการคำนวณจากการสร้างชุดคำถาม ที่ตอบ yes/no ก่อนคำนวณอัตราส่วนของ original text และบทสรุปจากคำตอบเดียวกัน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.confident-ai.com/blog/a-step-by-step-guide-to-evaluating-an-llm-text-summarization-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
