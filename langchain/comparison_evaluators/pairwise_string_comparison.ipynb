{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise string comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราสามารถทำการเปรียบผลลัพธ์ที่สองผลลัพธ์ โดยใช้โมเดลในการประเมินว่าผลลัพธ์ไหนที่ใกล้เคียงกับ reference ที่สุด เป็นการช่วยในการเปรียบเทียบผลลัพธ์จากโมเดลสองเดล หรือ prompt ที่ใช้แตกต่างกัน "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt-35-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"Assistant A provided the correct number of dogs in the park and followed the user's instructions to provide a specific number. The response is helpful, relevant, correct, and demonstrates depth by directly addressing the user's question with the accurate information.\\n\\nOn the other hand, Assistant B's response only provided a number without any context or relevance to the user's question. It lacks helpfulness, relevance, and depth.\\n\\nTherefore, Assistant A's response is better as it meets all the criteria and directly addresses the user's question with the correct information. \\n[[A]]\",\n",
       " 'value': 'A',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"there are three dogs\",\n",
    "    prediction_b=\"4\",\n",
    "    input=\"how many dogs are in the park?\",\n",
    "    reference=\"four\",\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it false you need to change to use gpt 4 instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตัวประเมิน pairwise string สามารถเรียกใช้งานได้โดยใช้เมธอด evaluate_string_pairs (หรือ async aevaluate_string_pairs) ซึ่งรับพารามิเตอร์ดังนี้:\n",
    "\n",
    "- prediction (str) – การตอบกลับที่คาดการณ์จากโมเดล แชลน หรือพรอมต์ตัวแรก\n",
    "- prediction_b (str) – การตอบกลับที่คาดการณ์จากโมเดล แชลน หรือพรอมต์ตัวที่สอง\n",
    "- input (str) – คำถาม พรอมต์ หรือข้อความอื่น ๆ ที่เป็นอินพุต\n",
    "- reference (str) – (สำหรับตัวแปร labeled_pairwise_string เท่านั้น) การตอบกลับที่เป็นข้อมูลอ้างอิง\n",
    "\n",
    "เมธอดเหล่านี้จะคืนค่าเป็นพจนานุกรมที่มีค่าดังนี้:\n",
    "\n",
    "- value: 'A' หรือ 'B' เพื่อระบุว่าชอบ prediction หรือ prediction_b ตามลำดับ\n",
    "- score: ค่าจำนวนเต็ม 0 หรือ 1 ที่แปลงมาจาก 'value' โดยที่คะแนน 1 หมายถึงชอบ prediction แรก และคะแนน 0 หมายถึงชอบ prediction_b\n",
    "- reasoning: ข้อความ \"การให้เหตุผลแบบ chain of thought\" จาก LLM ที่สร้างขึ้นก่อนการให้คะแนน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without References "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(\"pairwise_string\", llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"Assistant B provides a more helpful, insightful, and appropriate response by giving a clear definition of addition as a mathematical operation that adds two numbers to create a third number, the 'sum'. This response is more relevant as it refers to a real definition of addition from mathematics. It is also correct, accurate, and factual. Assistant A's response, on the other hand, only states that addition is a mathematical operation without providing further details. Therefore, Assistant B's response demonstrates more depth of thought by providing a more detailed explanation of addition. \\nTherefore, [[B]] is better.\",\n",
       " 'value': 'B',\n",
       " 'score': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Addition is a mathematical operation.\",\n",
    "    prediction_b=\"Addition is a mathematical operation that adds two numbers to create a third number, the 'sum'.\",\n",
    "    input=\"What is addition?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สามารถกำหนดเกณฑ์ในการประเมินได้โดย criteria ที่อยู่ในรูปของ dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "custom_criteria = {\n",
    "    \"simplicity\": \"Is the language straightforward and unpretentious?\",\n",
    "    \"clarity\": \"Are the sentences clear and easy to understand?\",\n",
    "    \"precision\": \"Is the writing precise, with no unnecessary words or details?\",\n",
    "    \"truthfulness\": \"Does the writing feel honest and sincere?\",\n",
    "    \"subtext\": \"Does the writing suggest deeper meanings or themes?\",\n",
    "}\n",
    "evaluator = load_evaluator(\"pairwise_string\", criteria=custom_criteria, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': \"Assistant B's response is overly complex and pretentious, using convoluted language and unnecessary details. It lacks simplicity, clarity, and precision, making it difficult to understand and appreciate. On the other hand, Assistant A's response is straightforward, clear, and honest. It presents the topic of families in a simple and truthful manner without unnecessary embellishments. Therefore, [[A]] is better.\",\n",
       " 'value': 'A',\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"Every cheerful household shares a similar rhythm of joy; but sorrow, in each household, plays a unique, haunting melody.\",\n",
    "    prediction_b=\"Where one finds a symphony of joy, every domicile of happiness resounds in harmonious,\"\n",
    "    \" identical notes; yet, every abode of despair conducts a dissonant orchestra, each\"\n",
    "    \" playing an elegy of grief that is peculiar and profound to its own existence.\",\n",
    "    input=\"Write some prose about families.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize the Evaluation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Given the input context, which do you prefer: A or B?\n",
    "Evaluate based on the following criteria:\n",
    "{criteria}\n",
    "Reason step by step and finally, respond with either [[A]] or [[B]] on its own line.\n",
    "\n",
    "DATA\n",
    "----\n",
    "input: {input}\n",
    "reference: {reference}\n",
    "A: {prediction}\n",
    "B: {prediction_b}\n",
    "---\n",
    "Reasoning:\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\", prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input', 'prediction', 'prediction_b', 'reference'] partial_variables={'criteria': 'For this evaluation, you should primarily consider the following criteria:\\nhelpfulness: Is the submission helpful, insightful, and appropriate?\\nrelevance: Is the submission referring to a real quote from the text?\\ncorrectness: Is the submission correct, accurate, and factual?\\ndepth: Does the submission demonstrate depth of thought?'} template='Given the input context, which do you prefer: A or B?\\nEvaluate based on the following criteria:\\n{criteria}\\nReason step by step and finally, respond with either [[A]] or [[B]] on its own line.\\n\\nDATA\\n----\\ninput: {input}\\nreference: {reference}\\nA: {prediction}\\nB: {prediction_b}\\n---\\nReasoning:\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "# The prompt was assigned to the evaluator\n",
    "print(evaluator.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=\"The dog that ate the ice cream was named fido.\",\n",
    "    prediction_b=\"The dog's name is spot\",\n",
    "    input=\"What is the name of the dog that ate the ice cream?\",\n",
    "    reference=\"The dog's name is fido\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
