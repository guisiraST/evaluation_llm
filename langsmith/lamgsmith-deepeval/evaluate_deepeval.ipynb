{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/deepeval/__init__.py:42: UserWarning: You are using deepeval version 0.21.42, however version 0.21.55 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = settings.LANGCHAIN_TRACING_V2\n",
    "os.environ['LANGCHAIN_API_KEY'] = settings.LANGCHAIN_API_KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for question-answer\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = [p.text for p in soup.find_all(\"p\")]\n",
    "full_text = \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deepeval section\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "    \n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    ")\n",
    "model_eval = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen answer section\n",
    "model_gen = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\")\n",
    "\n",
    "def answer_question_llm(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates answers to user questions based on a provided website text using OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    inputs (dict): A dictionary with a single key 'question', representing the user's question as a string.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a single key 'output', containing the generated answer as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=(f\"Answer user questions in 2-3 sentences about this context: \\n\\n\\n {full_text}\")),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    "    )\n",
    "    messages = chat_template.format_messages(text=inputs[\"question\"])\n",
    "\n",
    "    # Call OpenAI\n",
    "    response = model_gen.invoke(messages)\n",
    "\n",
    "    # Response in output dict\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-dbrx-qa-custom-eval-g-eval-e15db94f' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/3caf3425-c29e-4dfd-9054-b069fbc2a174/compare?selectedSessions=db53f139-d5c6-4128-a440-692263d5aaea\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c261c115000541e498f141ca533e8d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50944bbb8314f15bfea9ed559e40f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fb864db7704d18a17987f02bddbdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca940e885d340f0a8e582b385aaf2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f4bb07490b4c93ab81fb1d3ca00f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model = model_eval\n",
    ")\n",
    "\n",
    "def g_eval(run: Run, example: Example) -> dict:\n",
    "    # Get outputs\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    reference = example.outputs.get(\"answer\")\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=prediction,\n",
    "    expected_output=reference)\n",
    "    \n",
    "    metric.measure(test_case)\n",
    "    return {\"score\": metric.score, \"comment\": metric.reason}\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [g_eval]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question_llm,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-custom-eval-g-eval\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-4o\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  build dataset for evaluate summary task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Summary deepeval\"\n",
    "\n",
    "inputs = [\"\"\"\n",
    "The 'coverage score' is calculated as the percentage of assessment questions\n",
    "for which both the summary and the original document provide a 'yes' answer. This\n",
    "method ensures that the summary not only includes key information from the original\n",
    "text but also accurately represents it. A higher coverage score indicates a\n",
    "more comprehensive and faithful summary, signifying that the summary effectively\n",
    "encapsulates the crucial points and details from the original content.\n",
    "\"\"\"]\n",
    "\n",
    "outputs = [\"\"\"\n",
    "The coverage score quantifies how well a summary captures and\n",
    "accurately represents key information from the original text,\n",
    "with a higher score indicating greater comprehensiveness.\n",
    "\"\"\"]\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about DBRX model.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluate summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-qa-custom-eval-sum-1-35b8bb5c' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/766d5bd1-ebf7-4a0e-97c5-65aff8bfcbf2/compare?selectedSessions=af4729e3-6c2d-45d3-94ae-a91eabf0294d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44956d22a77549ed87ee6cf1d35338e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc714d47f184d1abed41c38c014476c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.metrics import SummarizationMetric\n",
    "metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=model_eval,\n",
    "    assessment_questions=[\n",
    "        \"Is the coverage score based on a percentage of 'yes' answers?\",\n",
    "        \"Does the score ensure the summary's accuracy with the source?\",\n",
    "        \"Does a higher score mean a more comprehensive summary?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "def summarize(run: Run, example: Example) -> dict:\n",
    "    # Get outputs\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=prediction)\n",
    "    \n",
    "    metric.measure(test_case)\n",
    "    return {\"score\": metric.score, \"comment\": metric.reason}\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [summarize]\n",
    "dataset_name = \"Summary deepeval\" #See detail in build_dataset.ipynb\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question_llm,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-qa-custom-eval-sum-1\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Relevancy + Faithfullness + Contextual Recall + Contextual Precision + Contextual Relevancy + Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    ")\n",
    "\n",
    "# Load\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deepeval section\n",
    "class AzureOpenAI(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Azure OpenAI Model\"\n",
    "    \n",
    "# Replace these with real values\n",
    "custom_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo\",\n",
    "    temperature = 0.0\n",
    ")\n",
    "model_eval = AzureOpenAI(model=custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG\n",
    "from langsmith import traceable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "class RagBot:\n",
    "    \n",
    "    def __init__(self, retriever):\n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        self._model_gen = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\", temperature = 0.0)\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self.retrieve_docs(question)\n",
    "        chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=(\"\"\"You are a helpful AI code assistant with expertise in LCEL.\"\n",
    "                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{similar}\"\"\")),\n",
    "            HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "        ]\n",
    "        )\n",
    "        messages = chat_template.format_messages(text=question)\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": self._model_gen.invoke(messages).content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-RAG-test-LCEL-RAG-Deepeval-2d8d3cef' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/e9f3f26b-8d24-4a6d-b39d-433836c8b3f7/compare?selectedSessions=d342a771-e37c-4edb-b956-d84106a3fabd\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f744100baf497691019b50aa3c15f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59de10c423224bbfaad59455a79b0cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996b092aeeb94f9b963af857f85b2641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a11834f9c8c49ef9d79d28e31b987ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350909c98da04db2b9d5d45e2316fb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac4b379a5cc43068ca480c9009a038a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a2b57fac1428fbfc9c26edd812c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbf79ef8fb54ac5ada74a15532137c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c46cd43b754d0f9f155d01d428cb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49f765507e84e9e8cba321699e2a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1c536b5a1941428757abccf9cf1d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca26e847a55943a5a211fea0c6a02696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d111841f45a4430bfeda05a6a6ac9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f938ab4e9f497189369f24c796a0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f0b13c17b24008981a526cd71beb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7ec2a8003940e7a5cd0b137fa47754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, ContextualPrecisionMetric, ContextualRecallMetric, ContextualRelevancyMetric\n",
    "\n",
    "def answerrelevancy(run: Run, example: Example) -> dict:\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=prediction\n",
    "    )\n",
    "\n",
    "    answerrelevancy_metric = AnswerRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=model_eval,\n",
    "        include_reason=True\n",
    "    )\n",
    "    answerrelevancy_metric.measure(test_case)\n",
    "    return {\"key\": \"answer relevancy\", \"score\": answerrelevancy_metric.score, \"comment\": answerrelevancy_metric.reason}\n",
    "\n",
    "def faithfulness(run: Run, example: Example) -> dict:\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    retrieval_context = run.outputs.get(\"contexts\")\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=prediction,\n",
    "        retrieval_context=retrieval_context\n",
    "    )\n",
    "    \n",
    "    faithfulness_metric = FaithfulnessMetric(\n",
    "        threshold=0.7,\n",
    "        model=model_eval,\n",
    "        include_reason=True\n",
    "    )\n",
    "    faithfulness_metric.measure(test_case)\n",
    "    return {\"key\": \"faithfulness\", \"score\": faithfulness_metric.score, \"comment\": faithfulness_metric.reason}\n",
    "\n",
    "def contextualprecision(run: Run, example: Example) -> dict:\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    reference =  example.outputs.get(\"answer\")\n",
    "    retrieval_context = run.outputs.get(\"contexts\")\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=prediction,\n",
    "        expected_output=reference,\n",
    "        retrieval_context=retrieval_context\n",
    "    )\n",
    "    \n",
    "    contextualprecision_metric = ContextualPrecisionMetric(\n",
    "        threshold=0.7,\n",
    "        model=model_eval,\n",
    "        include_reason=True\n",
    "    )\n",
    "    contextualprecision_metric.measure(test_case)\n",
    "    return {\"key\": \"contextual precision\", \"score\": contextualprecision_metric.score, \"comment\": contextualprecision_metric.reason}\n",
    "\n",
    "def contextualrecall(run: Run, example: Example) -> dict:\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    reference =  example.outputs.get(\"answer\"),\n",
    "    retrieval_context = run.outputs.get(\"contexts\")\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=prediction,\n",
    "        expected_output=reference,\n",
    "        retrieval_context=retrieval_context\n",
    "    )\n",
    "    \n",
    "    contextualrecall_metric = ContextualRecallMetric(\n",
    "        threshold=0.7,\n",
    "        model=model_eval,\n",
    "        include_reason=True\n",
    "    )\n",
    "    contextualrecall_metric.measure(test_case)\n",
    "    return {\"key\": \"contextual recall\", \"score\": contextualrecall_metric.score, \"comment\": contextualrecall_metric.reason}\n",
    "\n",
    "def contextualrelevancy(run: Run, example: Example) -> dict:\n",
    "    question = example.inputs.get(\"question\")\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    retrieval_context = run.outputs.get(\"contexts\")\n",
    "    test_case = LLMTestCase(\n",
    "        input=question,\n",
    "        actual_output=prediction,\n",
    "        retrieval_context=retrieval_context\n",
    "    )\n",
    "    contextualrelevancy_metric = ContextualRelevancyMetric(\n",
    "        threshold=0.7,\n",
    "        model=model_eval,\n",
    "        include_reason=True\n",
    "    )\n",
    "    contextualrelevancy_metric.measure(test_case)\n",
    "    return {\"key\": \"contextual relevancy\", \"score\": contextualrelevancy_metric.score, \"comment\": contextualrelevancy_metric.reason}\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [answerrelevancy, faithfulness, contextualprecision, contextualrecall, contextualrelevancy]\n",
    "dataset_name = \"RAG_test_LCEL\" \n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-RAG-test-LCEL-RAG-Deepeval\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
