{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can I define my own custom evaluator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](evaluation_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = settings.LANGCHAIN_TRACING_V2\n",
    "os.environ['LANGCHAIN_API_KEY'] = settings.LANGCHAIN_API_KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog post\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "text = [p.text for p in soup.find_all(\"p\")]\n",
    "full_text = \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "model_gen = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\")\n",
    "model_eval = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\")\n",
    "\n",
    "def answer_dbrx_question_llm(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Generates answers to user questions based on a provided website text using OpenAI API.\n",
    "\n",
    "    Parameters:\n",
    "    inputs (dict): A dictionary with a single key 'question', representing the user's question as a string.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with a single key 'output', containing the generated answer as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=(f\"Answer user questions in 2-3 sentences about this context: \\n\\n\\n {full_text}\")),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    "    )\n",
    "    messages = chat_template.format_messages(text=inputs[\"question\"])\n",
    "\n",
    "    # Call OpenAI\n",
    "    response = model_gen.invoke(messages)\n",
    "\n",
    "    # Response in output dict\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-dbrx-qa-custom-eval-is-answered-14b3c19c' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/3caf3425-c29e-4dfd-9054-b069fbc2a174/compare?selectedSessions=3f13b003-f0fb-42e6-9ffb-1fbc92b72839\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56a2b570895419caa9c159cb84ac17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "def is_answered(run: Run, example: Example) -> dict:\n",
    "    # Get outputs\n",
    "    student_answer = run.outputs.get(\"answer\")\n",
    "\n",
    "    # Check if the student_answer is an empty string\n",
    "    if not student_answer:\n",
    "        return {\"key\": \"is_answered\", \"score\": 0}\n",
    "    else:\n",
    "        return {\"key\": \"is_answered\", \"score\": 1}\n",
    "\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [is_answered]\n",
    "dataset_name = \"DBRX\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_dbrx_question_llm,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-custom-eval-is-answered\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
