{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = settings.LANGCHAIN_TRACING_V2\n",
    "# os.environ['LANGCHAIN_API_KEY'] = settings.LANGCHAIN_API_KEY\n",
    "\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "# os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "from api_key_config import settings\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ['LANGCHAIN_API_KEY'] = settings.LANGCHAIN_API_KEY \n",
    "\n",
    "azure_configs = {\n",
    "    \"base_url\": settings.AZURE_OPENAI_ENDPOINT,\n",
    "    \"model_deployment\": None,\n",
    "    \"model_name\": \"gpt-35-turbo\",\n",
    "    \"embedding_deployment\": None,\n",
    "    \"embedding_name\": \"text-embedding-ada-002\",  # most likely\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "azure_model = AzureChatOpenAI(\n",
    "    openai_api_version=settings.OPENAI_API_VERSION,\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=azure_configs[\"model_deployment\"],\n",
    "    model=azure_configs[\"model_name\"],\n",
    "    validate_base_url=False,\n",
    ")\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=settings.OPENAI_API_VERSION,\n",
    "    azure_endpoint=azure_configs[\"base_url\"],\n",
    "    azure_deployment=azure_configs[\"embedding_deployment\"],\n",
    "    model=azure_configs[\"embedding_name\"],\n",
    ")\n",
    "\n",
    "# Load\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=azure_embeddings)\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG\n",
    "from langsmith import traceable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "class RagBot:\n",
    "    \n",
    "    def __init__(self, retriever):\n",
    "        self._retriever = retriever\n",
    "        # Wrapping the client instruments the LLM\n",
    "        self._model_gen = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\", \n",
    "                                          openai_api_version=settings.OPENAI_API_VERSION,\n",
    "                                          azure_endpoint=settings.AZURE_OPENAI_ENDPOINT,\n",
    "                                          temperature = 0.0)\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        similar = self.retrieve_docs(question)\n",
    "        chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessage(content=(\"\"\"You are a helpful AI code assistant with expertise in LCEL.\"\n",
    "                    \" Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "                    f\"## Docs\\n\\n{similar}\"\"\")),\n",
    "            HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "        ]\n",
    "        )\n",
    "        messages = chat_template.format_messages(text=question)\n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": self._model_gen.invoke(messages).content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "\n",
    "rag_bot = RagBot(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG chain\n",
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-dbrx-qa-custom-eval-ragas-537bf2a2' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/e9f3f26b-8d24-4a6d-b39d-433836c8b3f7/compare?selectedSessions=a986ebbb-41d6-48e2-8ae8-d3f37c420306\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5c052322aa424ebb05da0f8b605514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8091f22e854f86bfb1a7940de5d1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1f23dabc5e4fad90abb3687e58988b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9234a96d911c4fb885e9c4304eb00eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "from datasets import Dataset, DatasetDict, Features, Value, Sequence\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "\n",
    "\n",
    "\n",
    "def ragas_eval(run: Run, example: Example) -> dict:\n",
    "    question = example.inputs.get(\"question\")\n",
    "    ground_truth = example.outputs.get(\"answer\")\n",
    "    answer = run.outputs.get(\"answer\")\n",
    "    contexts = run.outputs.get(\"contexts\")\n",
    "    # Define the schema\n",
    "    features = Features({\n",
    "        'question': Value('string'),\n",
    "        'ground_truth': Value('string'),\n",
    "        'answer': Value('string'),\n",
    "        'contexts': Sequence(Value('string')),\n",
    "    })\n",
    "    \n",
    "    custom_dataset = Dataset.from_dict({\"question\": [question],\n",
    "                                        \"ground_truth\": [ground_truth],\n",
    "                                        \"answer\": [answer],\n",
    "                                        \"contexts\": [contexts]},\n",
    "                                       features=features)\n",
    "    custom_dataset_dict = DatasetDict({\"eval\": custom_dataset})\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "        harmfulness,\n",
    "    ]\n",
    "\n",
    "    result = ragas_evaluate(\n",
    "        custom_dataset_dict[\"eval\"], metrics=metrics, llm=azure_model, embeddings=azure_embeddings\n",
    "    )\n",
    "\n",
    "    return {\n",
    "            \"results\": [\n",
    "                # Provide the key, score and other relevant information for each metric\n",
    "                {\"key\": \"faithfulness\", \"score\": result[\"faithfulness\"]},\n",
    "                {\"key\": \"answer_relevancy\", \"score\": result[\"answer_relevancy\"]},\n",
    "                {\"key\": \"context_recall\", \"score\": result[\"context_recall\"]},\n",
    "                {\"key\": \"context_precision\", \"score\": result[\"context_precision\"]},\n",
    "                {\"key\": \"harmfulness\", \"score\": result[\"harmfulness\"]},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [ragas_eval]\n",
    "dataset_name = \"RAG_test_LCEL\" \n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-dbrx-qa-custom-eval-ragas\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: Ragas lib maybe occur error sometime so i recommend you run on deepevall in each metric instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
