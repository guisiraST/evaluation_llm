{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/project/src/evaluate_llm/')\n",
    "\n",
    "from api_key_config import settings\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = settings.LANGCHAIN_TRACING_V2\n",
    "os.environ['LANGCHAIN_API_KEY'] = settings.LANGCHAIN_API_KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = settings.OPENAI_API_VERSION\n",
    "os.environ[\"OPENAI_API_KEY\"] = settings.OPENAI_API_KEY\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = settings.AZURE_OPENAI_ENDPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"Could you say: The quick brown fox jumps over the lazy dog\",\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"The quick brown dog jumps over the lazy fox\",\n",
    "]\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"stat_test\"\n",
    "\n",
    "# Store\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about statistic eval\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create generate text answer question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "model_gen = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\")\n",
    "model_eval = AzureChatOpenAI(azure_deployment=\"gpt-35-turbo\")\n",
    "\n",
    "def answer_question_llm(inputs: dict) -> dict:\n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=(f\"Answer user questions\")),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    "    )\n",
    "    messages = chat_template.format_messages(text=inputs[\"question\"])\n",
    "\n",
    "    # Call OpenAI\n",
    "    response = model_gen.invoke(messages)\n",
    "\n",
    "    # Response in output dict\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-bleu-score-8a86c4b6' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/17aefcee-326d-4e8e-85b3-1f890af4b74d/compare?selectedSessions=92253e8e-983b-486c-8d2c-f46d54ec4e8e\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4559434ab34eaf9cf3638e30026b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "def bleu_score(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    reference =  example.outputs.get(\"answer\")\n",
    "    score_1gram = sentence_bleu([reference.split()], prediction.split(), weights=(1, 0, 0, 0))\n",
    "    score_2gram = sentence_bleu([reference.split()], prediction.split(), weights=(0, 1, 0, 0))\n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\"key\": \"BLEU-1gram\", \"score\": score_1gram},\n",
    "            {\"key\": \"BLEU-2gram\", \"score\": score_2gram},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [bleu_score]\n",
    "dataset_name = \"stat_test\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question_llm,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-bleu-score\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-rouge-score-92fc4ca9' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/17aefcee-326d-4e8e-85b3-1f890af4b74d/compare?selectedSessions=4646fcc6-2931-4270-97c9-ef3041407c47\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3427359c33646c6b2b732c3a41a796f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_score(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    reference =  example.outputs.get(\"answer\")\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference,prediction)\n",
    "    return {\n",
    "        \"results\": [\n",
    "            {\"key\": \"rouge1_fmeasure\", \"score\": scores[\"rouge1\"].fmeasure},\n",
    "            {\"key\": \"rouge2_fmeasure\", \"score\": scores[\"rouge2\"].fmeasure},\n",
    "            {\"key\": \"rougeL_fmeasure\", \"score\": scores[\"rougeL\"].fmeasure},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [rouge_score]\n",
    "dataset_name = \"stat_test\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question_llm,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-rouge-score\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'test-meteor-score-2b26a352' at:\n",
      "https://smith.langchain.com/o/4da9684a-c78b-54bf-a119-2e143c6c11df/datasets/17aefcee-326d-4e8e-85b3-1f890af4b74d/compare?selectedSessions=943790f6-846a-4e2f-9276-d61737d9a6e1\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ba9106687a40d4aeb0d10557f55e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "from nltk.translate.meteor_score import meteor_score as meteor_scorer\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def meteor_score(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"answer\")\n",
    "    reference =  example.outputs.get(\"answer\")\n",
    "    score = meteor_scorer([word_tokenize(reference)], word_tokenize(prediction), alpha = 0.9, beta = 3, gamma = 0.5)\n",
    "    return {\"key\": \"meteor\", \"score\": score}\n",
    "\n",
    "# Evaluators\n",
    "qa_evalulator = [meteor_score]\n",
    "dataset_name = \"stat_test\"\n",
    "\n",
    "# Run\n",
    "experiment_results = evaluate(\n",
    "    answer_question_llm,\n",
    "    data=dataset_name,\n",
    "    evaluators=qa_evalulator,\n",
    "    experiment_prefix=\"test-meteor-score\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"stuff website context into gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
